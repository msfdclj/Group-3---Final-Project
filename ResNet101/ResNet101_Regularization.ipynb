{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout, Add, Activation, Input, BatchNormalization, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "train_set = h5py.File('train_signs.h5', 'r')\n",
    "train_x = np.array(train_set['train_set_x'][:])\n",
    "train_y = np.array(train_set['train_set_y'][:])\n",
    "classes = np.array(train_set['list_classes'].shape[0])\n",
    "\n",
    "test_set = h5py.File('test_signs.h5', 'r')\n",
    "test_x = np.array(test_set['test_set_x'][:])\n",
    "test_y = np.array(test_set['test_set_y'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of this picture is 5\n",
      "There are 6 classes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show our data\n",
    "plt.imshow(train_x[0])\n",
    "print('The label of this picture is', train_y[0])\n",
    "print('There are', classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset has 1080 pictures\n",
      "dataset has 120 pictures\n",
      "the shape of train data is (1080, 64, 64, 3)\n",
      "the shape of test data is (120, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "x_train = train_x / 255\n",
    "x_test = test_x / 255\n",
    "\n",
    "y_train = np_utils.to_categorical(train_y)\n",
    "y_test = np_utils.to_categorical(test_y)\n",
    "\n",
    "print('trainset has', x_train.shape[0], 'pictures')\n",
    "print('dataset has', x_test.shape[0], 'pictures')\n",
    "print('the shape of train data is', x_train.shape)\n",
    "print('the shape of test data is', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(x, f, k):\n",
    "    \n",
    "    f1, f2, f3 = f\n",
    "    x_shortcut = x\n",
    "    \n",
    "    #convolution path\n",
    "    x = Conv2D(filters = f1, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', kernel_regularizer = l2(0.03))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters = f2, kernel_size = (k, k), strides = (1, 1), padding = 'same', kernel_regularizer = l2(0.03))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters = f3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', kernel_regularizer = l2(0.03))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    \n",
    "    #add\n",
    "    x = Add()([x, x_shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(x, f, k, s):\n",
    "    \n",
    "    f1, f2, f3 = f\n",
    "    x_shortcut = x\n",
    "    \n",
    "    #convolution path\n",
    "    x = Conv2D(filters = f1, kernel_size = (1, 1), strides = (s, s), padding = 'valid', kernel_regularizer = l2(0.03))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters = f2, kernel_size = (k, k), strides = (1, 1), padding = 'same', kernel_regularizer = l2(0.03))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters = f3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', kernel_regularizer = l2(0.03))(x)\n",
    "    x = BatchNormalization(axis = 3)(x)\n",
    "    \n",
    "    #identity path\n",
    "    x_shortcut = Conv2D(filters = f3, kernel_size = (1, 1), strides = (s, s), padding = 'valid', kernel_regularizer = l2(0.03))(x_shortcut)\n",
    "    x_shortcut = BatchNormalization(axis = 3)(x_shortcut)\n",
    "    \n",
    "    #add\n",
    "    x = Add()([x, x_shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gzc/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   9472        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 15, 15, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 15, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 15, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 15, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 15, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 15, 15, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 15, 15, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 15, 15, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 15, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 256)  0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 15, 15, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 15, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 15, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 15, 15, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 15, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 15, 15, 256)  0           batch_normalization_8[0][0]      \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 15, 15, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 15, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 15, 15, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 15, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 15, 15, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15, 15, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 15, 15, 256)  0           batch_normalization_11[0][0]     \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 15, 15, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 128)    32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 512)    66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 512)    131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 512)    2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 512)    2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 512)    0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 128)    65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 128)    512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 128)    147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 512)    66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 512)    2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 512)    0           batch_normalization_18[0][0]     \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 128)    65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 128)    147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 512)    66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 128)    65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 512)    66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 512)    2048        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           batch_normalization_24[0][0]     \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 512)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 256)    131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 256)    1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 256)    590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 256)    1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 256)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 1024)   263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 1024)   525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 1024)   4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 1024)   4096        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 1024)   0           batch_normalization_27[0][0]     \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 256)    262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 256)    1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 256)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 256)    590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 256)    1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 256)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 1024)   263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 1024)   4096        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 1024)   0           batch_normalization_31[0][0]     \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 256)    262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 256)    1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 256)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 256)    590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 256)    1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 256)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 1024)   263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 1024)   4096        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_34[0][0]     \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 256)    262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 256)    1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 256)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 256)    590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 4, 4, 256)    1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 256)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 1024)   263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 4, 4, 1024)   4096        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_37[0][0]     \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 4, 256)    262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 4, 4, 256)    1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 256)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 4, 256)    590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 4, 4, 256)    1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 256)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 1024)   263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 1024)   4096        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_40[0][0]     \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 256)    262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 256)    1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 256)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 256)    590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 256)    1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 256)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 1024)   263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 1024)   4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_43[0][0]     \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 1024)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 256)    262400      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 256)    1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 256)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 256)    590080      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 256)    1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 256)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 1024)   263168      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 1024)   4096        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_46[0][0]     \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 1024)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 256)    262400      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 256)    1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 256)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 256)    590080      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 256)    1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 256)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 1024)   263168      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 1024)   4096        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_49[0][0]     \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 1024)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 256)    262400      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 256)    1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 256)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 256)    590080      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 256)    1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 256)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 1024)   263168      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 1024)   4096        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_52[0][0]     \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 1024)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 256)    262400      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 256)    1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 256)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 4, 4, 256)    590080      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 256)    1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 256)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 4, 4, 1024)   263168      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 1024)   4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_55[0][0]     \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 1024)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 4, 4, 256)    262400      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 4, 4, 256)    1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 256)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 4, 4, 256)    590080      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 4, 4, 256)    1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 4, 4, 256)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 4, 4, 1024)   263168      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 4, 4, 1024)   4096        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_58[0][0]     \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 4, 4, 1024)   0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 4, 4, 256)    262400      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 4, 4, 256)    1024        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 4, 4, 256)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 4, 4, 256)    590080      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 4, 4, 256)    1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 4, 4, 256)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 4, 4, 1024)   263168      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 4, 4, 1024)   4096        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_61[0][0]     \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 4, 4, 1024)   0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 4, 4, 256)    262400      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 4, 4, 256)    1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 4, 4, 256)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 4, 4, 256)    590080      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 4, 4, 256)    1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 4, 4, 256)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 4, 4, 1024)   263168      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 4, 4, 1024)   4096        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_64[0][0]     \n",
      "                                                                 activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 4, 4, 1024)   0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 4, 4, 256)    262400      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4, 4, 256)    1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 4, 4, 256)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 4, 4, 256)    590080      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 4, 4, 256)    1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 4, 4, 256)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 4, 4, 1024)   263168      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 4, 4, 1024)   4096        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_67[0][0]     \n",
      "                                                                 activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 4, 4, 1024)   0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 4, 4, 256)    262400      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 4, 4, 256)    1024        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 4, 4, 256)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 4, 4, 256)    590080      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 4, 4, 256)    1024        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 4, 4, 256)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 4, 4, 1024)   263168      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 4, 4, 1024)   4096        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_70[0][0]     \n",
      "                                                                 activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 4, 4, 1024)   0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 4, 4, 256)    262400      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 4, 4, 256)    1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 4, 4, 256)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 4, 4, 256)    590080      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 4, 4, 256)    1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 4, 4, 256)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 4, 4, 1024)   263168      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 4, 4, 1024)   4096        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_73[0][0]     \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 4, 4, 1024)   0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 4, 4, 256)    262400      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 4, 4, 256)    1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 4, 4, 256)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 4, 4, 256)    590080      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4, 4, 256)    1024        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 4, 4, 256)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 4, 4, 1024)   263168      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 4, 1024)   4096        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_76[0][0]     \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 4, 4, 1024)   0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 4, 4, 256)    262400      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 4, 256)    1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 4, 256)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 4, 4, 256)    590080      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 4, 256)    1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 4, 256)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 4, 1024)   263168      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 4, 1024)   4096        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_79[0][0]     \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 4, 1024)   0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 4, 4, 256)    262400      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 4, 256)    1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 4, 256)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 4, 256)    590080      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 4, 256)    1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 256)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 1024)   263168      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 4, 1024)   4096        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_82[0][0]     \n",
      "                                                                 activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 4, 1024)   0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 4, 256)    262400      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 4, 256)    1024        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 4, 256)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 256)    590080      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 4, 256)    1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 4, 256)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 1024)   263168      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 4, 1024)   4096        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_85[0][0]     \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 4, 1024)   0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 4, 256)    262400      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 4, 256)    1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 4, 256)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 256)    590080      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 4, 256)    1024        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 4, 256)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 4, 1024)   263168      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 4, 1024)   4096        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_88[0][0]     \n",
      "                                                                 activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 4, 1024)   0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 4, 256)    262400      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 4, 256)    1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 4, 256)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 4, 256)    590080      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 4, 256)    1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 4, 256)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 4, 1024)   263168      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 4, 1024)   4096        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_91[0][0]     \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 4, 1024)   0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 4, 256)    262400      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 4, 256)    1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 4, 256)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 4, 256)    590080      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 4, 4, 256)    1024        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 4, 256)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 4, 1024)   263168      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 4, 4, 1024)   4096        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_94[0][0]     \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 4, 4, 1024)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 2, 2, 512)    524800      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 2, 2, 512)    2048        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 2, 512)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 2, 2, 512)    2359808     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 2, 512)    2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 2, 512)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 2, 2, 2048)   2099200     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 2, 2048)   8192        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 2, 2048)   8192        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_97[0][0]     \n",
      "                                                                 batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 2, 2048)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 2, 2, 512)    1049088     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 2, 512)    2048        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 2, 512)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 2, 512)    2359808     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 2, 512)    2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 2, 512)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 2, 2048)   8192        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_101[0][0]    \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 2, 2048)   0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 2, 512)    1049088     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 2, 2, 512)    2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 2, 512)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 2, 512)    2359808     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 2, 2, 512)    2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 2, 512)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 2, 2, 2048)   8192        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_104[0][0]    \n",
      "                                                                 activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 2, 2, 2048)   0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 2048)   0           activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, array(6))     12294       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 42,670,470\n",
      "Trainable params: 42,565,126\n",
      "Non-trainable params: 105,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_input = Input(shape = (64, 64, 3))\n",
    "\n",
    "x = ZeroPadding2D((3, 3))(x_input)\n",
    "\n",
    "x = Conv2D(64, (7, 7), strides = (2, 2), kernel_regularizer = l2(0.03))(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "\n",
    "x = convolutional_block(x, [64, 64, 256], 3, 1)\n",
    "x = identity_block(x, [64, 64, 256], 3)\n",
    "x = identity_block(x, [64, 64, 256], 3)\n",
    "\n",
    "x = convolutional_block(x, [128, 128, 512], 3, 2)\n",
    "x = identity_block(x, [128, 128, 512], 3)\n",
    "x = identity_block(x, [128, 128, 512], 3)\n",
    "x = identity_block(x, [128, 128, 512], 3)\n",
    "\n",
    "x = convolutional_block(x, [256, 256, 1024], 3, 2)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "x = identity_block(x, [256, 256, 1024], 3)\n",
    "\n",
    "x = convolutional_block(x, [512, 512, 2048], 3, 2)\n",
    "x = identity_block(x, [512, 512, 2048], 3)\n",
    "x = identity_block(x, [512, 512, 2048], 3)\n",
    "\n",
    "x = AveragePooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(classes, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = x_input, outputs = x)\n",
    "\n",
    "model.summary()\n",
    "model.save(\"resnet101_regu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 918 samples, validate on 162 samples\n",
      "Epoch 1/200\n",
      "918/918 [==============================] - 119s 130ms/step - loss: 4.7564 - acc: 1.0000 - val_loss: 5.0764 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93210, saving model to resnet101_800.h5\n",
      "Epoch 2/200\n",
      "918/918 [==============================] - 45s 50ms/step - loss: 4.6577 - acc: 1.0000 - val_loss: 4.8853 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.93210 to 0.93827, saving model to resnet101_800.h5\n",
      "Epoch 3/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 4.5569 - acc: 1.0000 - val_loss: 4.7745 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.93827\n",
      "Epoch 4/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 4.4576 - acc: 1.0000 - val_loss: 4.6838 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.93827\n",
      "Epoch 5/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 4.3618 - acc: 1.0000 - val_loss: 4.5862 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.93827\n",
      "Epoch 6/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 4.2696 - acc: 1.0000 - val_loss: 4.5680 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.93827\n",
      "Epoch 7/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 4.1819 - acc: 1.0000 - val_loss: 4.4979 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.93827\n",
      "Epoch 8/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 4.0963 - acc: 1.0000 - val_loss: 4.3730 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.93827\n",
      "Epoch 9/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 4.0131 - acc: 1.0000 - val_loss: 4.3230 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.93827\n",
      "Epoch 10/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.9347 - acc: 1.0000 - val_loss: 4.2951 - val_acc: 0.8827\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93827\n",
      "Epoch 11/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.9280 - acc: 0.9880 - val_loss: 4.6849 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.93827\n",
      "Epoch 12/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8926 - acc: 0.9935 - val_loss: 4.4674 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.93827\n",
      "Epoch 13/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8686 - acc: 0.9989 - val_loss: 4.5640 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93827\n",
      "Epoch 14/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8666 - acc: 0.9935 - val_loss: 4.4875 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.93827\n",
      "Epoch 15/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8587 - acc: 0.9956 - val_loss: 4.5899 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93827\n",
      "Epoch 16/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8394 - acc: 0.9978 - val_loss: 4.7024 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.93827\n",
      "Epoch 17/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8308 - acc: 0.9978 - val_loss: 4.5073 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.93827\n",
      "Epoch 18/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8158 - acc: 1.0000 - val_loss: 4.4614 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.93827\n",
      "Epoch 19/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.8038 - acc: 1.0000 - val_loss: 4.3906 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.93827\n",
      "Epoch 20/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 3.7929 - acc: 1.0000 - val_loss: 4.3295 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.93827\n",
      "Epoch 21/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7817 - acc: 1.0000 - val_loss: 4.2801 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.93827\n",
      "Epoch 22/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7703 - acc: 1.0000 - val_loss: 4.2461 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.93827\n",
      "Epoch 23/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7587 - acc: 1.0000 - val_loss: 4.2149 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.93827\n",
      "Epoch 24/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7466 - acc: 1.0000 - val_loss: 4.1946 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.93827\n",
      "Epoch 25/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7343 - acc: 1.0000 - val_loss: 4.1748 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.93827\n",
      "Epoch 26/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7219 - acc: 1.0000 - val_loss: 4.1539 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.93827\n",
      "Epoch 27/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.7091 - acc: 1.0000 - val_loss: 4.1366 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.93827\n",
      "Epoch 28/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6961 - acc: 1.0000 - val_loss: 4.1202 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.93827\n",
      "Epoch 29/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6829 - acc: 1.0000 - val_loss: 4.1023 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.93827\n",
      "Epoch 30/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6694 - acc: 1.0000 - val_loss: 4.0869 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.93827\n",
      "Epoch 31/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6557 - acc: 1.0000 - val_loss: 4.0705 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.93827\n",
      "Epoch 32/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6420 - acc: 1.0000 - val_loss: 4.0438 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.93827\n",
      "Epoch 33/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6281 - acc: 1.0000 - val_loss: 4.0154 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.93827\n",
      "Epoch 34/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.6133 - acc: 1.0000 - val_loss: 4.0166 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.93827\n",
      "Epoch 35/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5987 - acc: 1.0000 - val_loss: 3.9950 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.93827\n",
      "Epoch 36/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5839 - acc: 1.0000 - val_loss: 3.9746 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.93827\n",
      "Epoch 37/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5688 - acc: 1.0000 - val_loss: 3.9592 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.93827\n",
      "Epoch 38/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5535 - acc: 1.0000 - val_loss: 3.9397 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.93827\n",
      "Epoch 39/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5380 - acc: 1.0000 - val_loss: 3.9210 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.93827\n",
      "Epoch 40/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5223 - acc: 1.0000 - val_loss: 3.9026 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.93827\n",
      "Epoch 41/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.5064 - acc: 1.0000 - val_loss: 3.8861 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.93827\n",
      "Epoch 42/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.4903 - acc: 1.0000 - val_loss: 3.8664 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.93827\n",
      "Epoch 43/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.4741 - acc: 1.0000 - val_loss: 3.8437 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.93827\n",
      "Epoch 44/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.4575 - acc: 1.0000 - val_loss: 3.8267 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.93827\n",
      "Epoch 45/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.4409 - acc: 1.0000 - val_loss: 3.8088 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.93827\n",
      "Epoch 46/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.4240 - acc: 1.0000 - val_loss: 3.7889 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.93827\n",
      "Epoch 47/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.4069 - acc: 1.0000 - val_loss: 3.7701 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.93827\n",
      "Epoch 48/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.3898 - acc: 1.0000 - val_loss: 3.7589 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.93827\n",
      "Epoch 49/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.3722 - acc: 1.0000 - val_loss: 3.7387 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.93827\n",
      "Epoch 50/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.3546 - acc: 1.0000 - val_loss: 3.7177 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.93827\n",
      "Epoch 51/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.3369 - acc: 1.0000 - val_loss: 3.6945 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.93827\n",
      "Epoch 52/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.3189 - acc: 1.0000 - val_loss: 3.6704 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.93827\n",
      "Epoch 53/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.3008 - acc: 1.0000 - val_loss: 3.6510 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.93827\n",
      "Epoch 54/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.2825 - acc: 1.0000 - val_loss: 3.6253 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.93827\n",
      "Epoch 55/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.2640 - acc: 1.0000 - val_loss: 3.6005 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.93827\n",
      "Epoch 56/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.2455 - acc: 1.0000 - val_loss: 3.5786 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.93827\n",
      "Epoch 57/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.2267 - acc: 1.0000 - val_loss: 3.5559 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.93827\n",
      "Epoch 58/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.2078 - acc: 1.0000 - val_loss: 3.5318 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.93827\n",
      "Epoch 59/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 3.1887 - acc: 1.0000 - val_loss: 3.5070 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.93827\n",
      "Epoch 60/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.1695 - acc: 1.0000 - val_loss: 3.4829 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.93827\n",
      "Epoch 61/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.1502 - acc: 1.0000 - val_loss: 3.4584 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.93827\n",
      "Epoch 62/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.1307 - acc: 1.0000 - val_loss: 3.4349 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.93827\n",
      "Epoch 63/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.1111 - acc: 1.0000 - val_loss: 3.4116 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.93827\n",
      "Epoch 64/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.0913 - acc: 1.0000 - val_loss: 3.3929 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.93827\n",
      "Epoch 65/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.0715 - acc: 1.0000 - val_loss: 3.3706 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.93827\n",
      "Epoch 66/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.0515 - acc: 1.0000 - val_loss: 3.3465 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.93827\n",
      "Epoch 67/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 3.0313 - acc: 1.0000 - val_loss: 3.3248 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.93827\n",
      "Epoch 68/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 3.0111 - acc: 1.0000 - val_loss: 3.3001 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.93827\n",
      "Epoch 69/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 2.9908 - acc: 1.0000 - val_loss: 3.2792 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.93827\n",
      "Epoch 70/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 2.9703 - acc: 1.0000 - val_loss: 3.2604 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.93827\n",
      "Epoch 71/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.9498 - acc: 1.0000 - val_loss: 3.2384 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.93827\n",
      "Epoch 72/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.9290 - acc: 1.0000 - val_loss: 3.2119 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.93827\n",
      "Epoch 73/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.9083 - acc: 1.0000 - val_loss: 3.1911 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.93827\n",
      "Epoch 74/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 2.8874 - acc: 1.0000 - val_loss: 3.1622 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.93827\n",
      "Epoch 75/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.8664 - acc: 1.0000 - val_loss: 3.1381 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.93827\n",
      "Epoch 76/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.8454 - acc: 1.0000 - val_loss: 3.1094 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.93827\n",
      "Epoch 77/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.8243 - acc: 1.0000 - val_loss: 3.0946 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.93827\n",
      "Epoch 78/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 2.8030 - acc: 1.0000 - val_loss: 3.0701 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.93827\n",
      "Epoch 79/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.7817 - acc: 1.0000 - val_loss: 3.0472 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.93827\n",
      "Epoch 80/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.7603 - acc: 1.0000 - val_loss: 3.0235 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.93827\n",
      "Epoch 81/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.7389 - acc: 1.0000 - val_loss: 3.0011 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.93827\n",
      "Epoch 82/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.7174 - acc: 1.0000 - val_loss: 2.9779 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.93827\n",
      "Epoch 83/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.6958 - acc: 1.0000 - val_loss: 2.9560 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.93827\n",
      "Epoch 84/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.6742 - acc: 1.0000 - val_loss: 2.9376 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.93827\n",
      "Epoch 85/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.6525 - acc: 1.0000 - val_loss: 2.9118 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.93827\n",
      "Epoch 86/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.6307 - acc: 1.0000 - val_loss: 2.8857 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.93827\n",
      "Epoch 87/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.6089 - acc: 1.0000 - val_loss: 2.8636 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.93827\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918/918 [==============================] - 39s 42ms/step - loss: 2.5870 - acc: 1.0000 - val_loss: 2.8395 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.93827\n",
      "Epoch 89/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.5651 - acc: 1.0000 - val_loss: 2.8152 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.93827\n",
      "Epoch 90/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.5432 - acc: 1.0000 - val_loss: 2.7879 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.93827\n",
      "Epoch 91/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.5213 - acc: 1.0000 - val_loss: 2.7633 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.93827\n",
      "Epoch 92/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.4993 - acc: 1.0000 - val_loss: 2.7424 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.93827\n",
      "Epoch 93/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.4772 - acc: 1.0000 - val_loss: 2.7227 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.93827\n",
      "Epoch 94/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.4552 - acc: 1.0000 - val_loss: 2.6979 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.93827\n",
      "Epoch 95/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.4331 - acc: 1.0000 - val_loss: 2.6722 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.93827\n",
      "Epoch 96/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.4110 - acc: 1.0000 - val_loss: 2.6491 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.93827\n",
      "Epoch 97/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.3889 - acc: 1.0000 - val_loss: 2.6246 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.93827\n",
      "Epoch 98/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.3668 - acc: 1.0000 - val_loss: 2.6005 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.93827\n",
      "Epoch 99/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.3447 - acc: 1.0000 - val_loss: 2.5761 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.93827\n",
      "Epoch 100/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.3226 - acc: 1.0000 - val_loss: 2.5556 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.93827\n",
      "Epoch 101/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.3005 - acc: 1.0000 - val_loss: 2.5325 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.93827\n",
      "Epoch 102/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.2784 - acc: 1.0000 - val_loss: 2.5080 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.93827\n",
      "Epoch 103/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.2563 - acc: 1.0000 - val_loss: 2.4910 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.93827\n",
      "Epoch 104/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.2342 - acc: 1.0000 - val_loss: 2.4684 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.93827\n",
      "Epoch 105/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 2.2121 - acc: 1.0000 - val_loss: 2.4386 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.93827\n",
      "Epoch 106/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.1901 - acc: 1.0000 - val_loss: 2.4190 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.93827\n",
      "Epoch 107/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.1680 - acc: 1.0000 - val_loss: 2.3939 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.93827\n",
      "Epoch 108/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.1461 - acc: 1.0000 - val_loss: 2.3744 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.93827\n",
      "Epoch 109/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.1241 - acc: 1.0000 - val_loss: 2.3557 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.93827\n",
      "Epoch 110/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.1021 - acc: 1.0000 - val_loss: 2.3323 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.93827\n",
      "Epoch 111/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.0803 - acc: 1.0000 - val_loss: 2.3118 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.93827\n",
      "Epoch 112/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.0584 - acc: 1.0000 - val_loss: 2.2863 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.93827\n",
      "Epoch 113/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.0366 - acc: 1.0000 - val_loss: 2.2552 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.93827\n",
      "Epoch 114/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.0148 - acc: 1.0000 - val_loss: 2.2326 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.93827\n",
      "Epoch 115/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9932 - acc: 1.0000 - val_loss: 2.2246 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.93827\n",
      "Epoch 116/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9728 - acc: 1.0000 - val_loss: 2.2256 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.93827\n",
      "Epoch 117/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 2.0771 - acc: 0.9673 - val_loss: 13.6648 - val_acc: 0.2346\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.93827\n",
      "Epoch 118/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.0840 - acc: 0.9641 - val_loss: 5.2502 - val_acc: 0.5679\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.93827\n",
      "Epoch 119/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 2.0736 - acc: 0.9815 - val_loss: 6.7165 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.93827\n",
      "Epoch 120/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9914 - acc: 0.9967 - val_loss: 3.9195 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.93827\n",
      "Epoch 121/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9834 - acc: 0.9967 - val_loss: 3.2011 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.93827\n",
      "Epoch 122/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9726 - acc: 0.9978 - val_loss: 2.7483 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.93827\n",
      "Epoch 123/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9663 - acc: 0.9989 - val_loss: 2.5474 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.93827\n",
      "Epoch 124/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9587 - acc: 1.0000 - val_loss: 2.4761 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.93827\n",
      "Epoch 125/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9533 - acc: 1.0000 - val_loss: 2.4388 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.93827\n",
      "Epoch 126/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9524 - acc: 0.9989 - val_loss: 2.4268 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.93827\n",
      "Epoch 127/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9443 - acc: 1.0000 - val_loss: 2.3956 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.93827\n",
      "Epoch 128/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9403 - acc: 0.9989 - val_loss: 2.3726 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.93827\n",
      "Epoch 129/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9343 - acc: 1.0000 - val_loss: 2.3594 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.93827\n",
      "Epoch 130/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9295 - acc: 1.0000 - val_loss: 2.3471 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.93827\n",
      "Epoch 131/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9248 - acc: 1.0000 - val_loss: 2.3442 - val_acc: 0.9198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00131: val_acc did not improve from 0.93827\n",
      "Epoch 132/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9200 - acc: 1.0000 - val_loss: 2.3446 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.93827\n",
      "Epoch 133/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.9154 - acc: 1.0000 - val_loss: 2.3437 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.93827\n",
      "Epoch 134/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9106 - acc: 1.0000 - val_loss: 2.3421 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.93827\n",
      "Epoch 135/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9059 - acc: 1.0000 - val_loss: 2.3407 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.93827\n",
      "Epoch 136/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.9010 - acc: 1.0000 - val_loss: 2.3368 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.93827\n",
      "Epoch 137/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.8962 - acc: 1.0000 - val_loss: 2.3323 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.93827\n",
      "Epoch 138/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8915 - acc: 1.0000 - val_loss: 2.3253 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.93827\n",
      "Epoch 139/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8867 - acc: 1.0000 - val_loss: 2.3166 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.93827\n",
      "Epoch 140/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8818 - acc: 1.0000 - val_loss: 2.3068 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.93827\n",
      "Epoch 141/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8769 - acc: 1.0000 - val_loss: 2.3001 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.93827\n",
      "Epoch 142/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.8720 - acc: 1.0000 - val_loss: 2.2932 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.93827\n",
      "Epoch 143/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.8671 - acc: 1.0000 - val_loss: 2.2879 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.93827\n",
      "Epoch 144/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8622 - acc: 1.0000 - val_loss: 2.2824 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.93827\n",
      "Epoch 145/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8571 - acc: 1.0000 - val_loss: 2.2753 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.93827\n",
      "Epoch 146/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.8521 - acc: 1.0000 - val_loss: 2.2691 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.93827\n",
      "Epoch 147/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8470 - acc: 1.0000 - val_loss: 2.2623 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.93827\n",
      "Epoch 148/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8419 - acc: 1.0000 - val_loss: 2.2564 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.93827\n",
      "Epoch 149/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.8369 - acc: 1.0000 - val_loss: 2.2515 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.93827\n",
      "Epoch 150/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8317 - acc: 1.0000 - val_loss: 2.2447 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.93827\n",
      "Epoch 151/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.8264 - acc: 1.0000 - val_loss: 2.2379 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.93827\n",
      "Epoch 152/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8212 - acc: 1.0000 - val_loss: 2.2321 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.93827\n",
      "Epoch 153/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8159 - acc: 1.0000 - val_loss: 2.2276 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.93827\n",
      "Epoch 154/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.8105 - acc: 1.0000 - val_loss: 2.2219 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.93827\n",
      "Epoch 155/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.8052 - acc: 1.0000 - val_loss: 2.2171 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.93827\n",
      "Epoch 156/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7998 - acc: 1.0000 - val_loss: 2.2110 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.93827\n",
      "Epoch 157/200\n",
      "918/918 [==============================] - 39s 42ms/step - loss: 1.7943 - acc: 1.0000 - val_loss: 2.2058 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.93827\n",
      "Epoch 158/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7887 - acc: 1.0000 - val_loss: 2.2012 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.93827\n",
      "Epoch 159/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7832 - acc: 1.0000 - val_loss: 2.1933 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.93827\n",
      "Epoch 160/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7776 - acc: 1.0000 - val_loss: 2.1886 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.93827\n",
      "Epoch 161/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7719 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.93827\n",
      "Epoch 162/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7662 - acc: 1.0000 - val_loss: 2.1715 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.93827\n",
      "Epoch 163/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7604 - acc: 1.0000 - val_loss: 2.1642 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.93827\n",
      "Epoch 164/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.7547 - acc: 1.0000 - val_loss: 2.1580 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.93827\n",
      "Epoch 165/200\n",
      "918/918 [==============================] - 41s 44ms/step - loss: 1.7488 - acc: 1.0000 - val_loss: 2.1513 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.93827\n",
      "Epoch 166/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7430 - acc: 1.0000 - val_loss: 2.1463 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.93827\n",
      "Epoch 167/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7369 - acc: 1.0000 - val_loss: 2.1410 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.93827\n",
      "Epoch 168/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.7309 - acc: 1.0000 - val_loss: 2.1322 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.93827\n",
      "Epoch 169/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.7249 - acc: 1.0000 - val_loss: 2.1236 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.93827\n",
      "Epoch 170/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.7188 - acc: 1.0000 - val_loss: 2.1168 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.93827\n",
      "Epoch 171/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.7126 - acc: 1.0000 - val_loss: 2.1117 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.93827\n",
      "Epoch 172/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.7064 - acc: 1.0000 - val_loss: 2.1023 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.93827\n",
      "Epoch 173/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.7001 - acc: 1.0000 - val_loss: 2.0908 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.93827\n",
      "Epoch 174/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.6938 - acc: 1.0000 - val_loss: 2.0795 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.93827\n",
      "Epoch 175/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.6875 - acc: 1.0000 - val_loss: 2.0707 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.93827\n",
      "Epoch 176/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.6811 - acc: 1.0000 - val_loss: 2.0687 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.93827\n",
      "Epoch 177/200\n",
      "918/918 [==============================] - 41s 44ms/step - loss: 1.6746 - acc: 1.0000 - val_loss: 2.0602 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.93827\n",
      "Epoch 178/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.6681 - acc: 1.0000 - val_loss: 2.0598 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.93827\n",
      "Epoch 179/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.6614 - acc: 1.0000 - val_loss: 2.0458 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.93827\n",
      "Epoch 180/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.6548 - acc: 1.0000 - val_loss: 2.0321 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.93827\n",
      "Epoch 181/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.6481 - acc: 1.0000 - val_loss: 2.0209 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.93827\n",
      "Epoch 182/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.6414 - acc: 1.0000 - val_loss: 2.0106 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.93827\n",
      "Epoch 183/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.6346 - acc: 1.0000 - val_loss: 2.0001 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.93827\n",
      "Epoch 184/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.6277 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.93827\n",
      "Epoch 185/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.6208 - acc: 1.0000 - val_loss: 1.9804 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.93827\n",
      "Epoch 186/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.6138 - acc: 1.0000 - val_loss: 1.9703 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.93827\n",
      "Epoch 187/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.6069 - acc: 1.0000 - val_loss: 1.9624 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.93827\n",
      "Epoch 188/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5997 - acc: 1.0000 - val_loss: 1.9501 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.93827\n",
      "Epoch 189/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5926 - acc: 1.0000 - val_loss: 1.9402 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.93827\n",
      "Epoch 190/200\n",
      "918/918 [==============================] - 40s 44ms/step - loss: 1.5855 - acc: 1.0000 - val_loss: 1.9311 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.93827\n",
      "Epoch 191/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.5783 - acc: 1.0000 - val_loss: 1.9216 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.93827\n",
      "Epoch 192/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5710 - acc: 1.0000 - val_loss: 1.9123 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.93827\n",
      "Epoch 193/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5637 - acc: 1.0000 - val_loss: 1.9114 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.93827\n",
      "Epoch 194/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.5562 - acc: 1.0000 - val_loss: 1.9051 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.93827\n",
      "Epoch 195/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.5488 - acc: 1.0000 - val_loss: 1.8935 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.93827\n",
      "Epoch 196/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5413 - acc: 1.0000 - val_loss: 1.8806 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.93827\n",
      "Epoch 197/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5337 - acc: 1.0000 - val_loss: 1.8709 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.93827\n",
      "Epoch 198/200\n",
      "918/918 [==============================] - 39s 43ms/step - loss: 1.5261 - acc: 1.0000 - val_loss: 1.8643 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.93827\n",
      "Epoch 199/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.5185 - acc: 1.0000 - val_loss: 1.8540 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.93827\n",
      "Epoch 200/200\n",
      "918/918 [==============================] - 40s 43ms/step - loss: 1.5108 - acc: 1.0000 - val_loss: 1.8485 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.93827\n",
      "Total time:  8425.738776922226\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "model = keras.models.load_model(\"resnet101_regu.h5\")\n",
    "# After every 200 epochs, we manually changed the learning rate and run this cell again\n",
    "adam = optimizers.Adam(lr=0.0002)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"resnet101_regu.h5\", monitor='val_acc', verbose=1, mode='max', save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(x_train, y_train, validation_split=0.15, epochs=200, batch_size=32, callbacks=[checkpoint])\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 46s 386ms/step\n",
      "loss is 4.780823548634847\n",
      "accuracy is 0.9416666706403096\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"resnet101_regu.h5\")\n",
    "pred = model.evaluate(x_test, y_test)\n",
    "print('loss is', pred[0])\n",
    "print('accuracy is', pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('r101_regu_loss.npy', history.history['loss'])\n",
    "np.save('r101_regu_acc.npy', history.history['acc'])\n",
    "np.save('r101_regu_vloss.npy', history.history['val_loss'])\n",
    "np.save('r101_regu_vacc.npy', history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
